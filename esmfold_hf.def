# ESMFold HuggingFace Singularity Definition File
#
# Build with: singularity build --fakeroot esmfold_hf.sif esmfold_hf.def
# Or without fakeroot: sudo singularity build esmfold_hf.sif esmfold_hf.def
#
# Run with GPU: singularity run --nv esmfold_hf.sif
# Run inference: singularity run --nv --bind /data:/data esmfold_hf.sif esm-fold-hf -i /data/input.fasta -o /data/output
#
# IMPORTANT: Build from the esm directory where esm/, scripts/, setup_hf.py exist

Bootstrap: docker
From: nvidia/cuda:12.4.0-runtime-ubuntu22.04

%labels
    Author ESM HuggingFace Container
    Version 2.2.0-hf
    Description ESMFold with HuggingFace Transformers - Singularity Build (PyTorch 2.6, NumPy 1.26.4)

%files
    # Copy ESM source code into container
    esm /workspace/esm
    scripts /workspace/scripts
    setup_hf.py /workspace/setup.py
    README.md /workspace/README.md
    LICENSE /workspace/LICENSE

%environment
    export PYTHONUNBUFFERED=1
    export HF_HOME=/tmp/.cache/huggingface
    export TRANSFORMERS_CACHE=/tmp/.cache/huggingface/transformers
    export TORCH_HOME=/tmp/.cache/torch
    export PATH=/usr/local/bin:$PATH
    export LC_ALL=C.UTF-8
    export LANG=C.UTF-8

%post
    # Prevent interactive prompts
    export DEBIAN_FRONTEND=noninteractive
    export TZ=UTC

    # Install system dependencies
    apt-get update && apt-get install -y --no-install-recommends \
        python3.10 \
        python3.10-dev \
        python3.10-venv \
        python3-pip \
        git \
        wget \
        curl \
        ca-certificates \
        && rm -rf /var/lib/apt/lists/*

    # Set up python symlinks
    ln -sf /usr/bin/python3.10 /usr/bin/python
    ln -sf /usr/bin/python3.10 /usr/bin/python3
    ln -sf /usr/bin/pip3 /usr/bin/pip

    # Upgrade pip
    python -m pip install --no-cache-dir --upgrade pip setuptools wheel

    # Install NumPy 1.26.4 FIRST (before PyTorch to ensure correct version)
    pip install --no-cache-dir "numpy==1.26.4"

    # Install PyTorch 2.6.0 with CUDA 12.4 support (required by transformers for CVE-2025-32434)
    pip install --no-cache-dir \
        torch==2.6.0 \
        torchvision==0.21.0 \
        torchaudio==2.6.0 \
        --index-url https://download.pytorch.org/whl/cu124

    # Verify NumPy version wasn't overwritten
    pip install --no-cache-dir "numpy==1.26.4" --force-reinstall

    # Install HuggingFace dependencies
    pip install --no-cache-dir \
        "transformers>=4.44.0,<4.58.0" \
        "accelerate>=0.20.0" \
        "biopython" \
        "scipy" \
        "einops"

    # Install ESM package with HuggingFace support (files copied via %files)
    cd /workspace && pip install --no-cache-dir -e ".[esmfold_hf]"

    # Verify esm-fold-hf is installed
    which esm-fold-hf || echo "Warning: esm-fold-hf not in PATH"

    # Create test installation script
    cat > /workspace/test_installation.py << 'TESTEOF'
#!/usr/bin/env python
import sys

print("=" * 50)
print("ESMFold HuggingFace Installation Test")
print("=" * 50)
print(f"Python version: {sys.version.split()[0]}")

import numpy as np
print(f"NumPy version: {np.__version__}")

import torch
print(f"PyTorch version: {torch.__version__}")
print(f"CUDA available: {torch.cuda.is_available()}")
if torch.cuda.is_available():
    print(f"CUDA version: {torch.version.cuda}")
    print(f"GPU: {torch.cuda.get_device_name(0)}")
    print(f"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB")

print("\nTesting HuggingFace transformers...")
try:
    from transformers import __version__ as tf_version
    print(f"Transformers version: {tf_version}")
    print("\nLoading ESMFold tokenizer...")
    from transformers import AutoTokenizer
    tokenizer = AutoTokenizer.from_pretrained("facebook/esmfold_v1")
    print("Tokenizer loaded successfully!")
    print("\nNote: Model will be downloaded on first use (~10GB)")
    print("\n" + "=" * 50)
    print("Installation test PASSED!")
    print("=" * 50)
except Exception as e:
    print(f"\nError: {e}")
    sys.exit(1)
TESTEOF
    chmod +x /workspace/test_installation.py

    # Create inference script
    cat > /workspace/inference.py << 'INFEREOF'
#!/usr/bin/env python
"""Simple ESMFold HF inference script
Usage: python inference.py <sequence> [--output output.pdb]
"""
import argparse
import torch
from transformers import AutoTokenizer, EsmForProteinFolding
from transformers.models.esm.openfold_utils.protein import to_pdb, Protein as OFProtein
from transformers.models.esm.openfold_utils.feats import atom14_to_atom37

def convert_outputs_to_pdb(outputs):
    final_atom_positions = atom14_to_atom37(outputs["positions"][-1], outputs)
    outputs = {k: v.to("cpu").numpy() for k, v in outputs.items()}
    final_atom_positions = final_atom_positions.cpu().numpy()
    final_atom_mask = outputs["atom37_atom_exists"]

    aa = outputs["aatype"][0]
    pred_pos = final_atom_positions[0]
    mask = final_atom_mask[0]
    resid = outputs["residue_index"][0] + 1
    pred = OFProtein(
        aatype=aa,
        atom_positions=pred_pos,
        atom_mask=mask,
        residue_index=resid,
        b_factors=outputs["plddt"][0],
        chain_index=outputs["chain_index"][0] if "chain_index" in outputs else None,
    )
    return to_pdb(pred)

def main():
    parser = argparse.ArgumentParser(description="Run ESMFold inference")
    parser.add_argument("sequence", type=str, help="Protein sequence")
    parser.add_argument("--output", type=str, default="output.pdb", help="Output PDB file")
    parser.add_argument("--cpu", action="store_true", help="Use CPU instead of GPU")
    parser.add_argument("--fp16", action="store_true", help="Use half precision")
    args = parser.parse_args()

    print(f"Loading ESMFold model from HuggingFace...")
    tokenizer = AutoTokenizer.from_pretrained("facebook/esmfold_v1")
    model = EsmForProteinFolding.from_pretrained("facebook/esmfold_v1")

    if args.fp16 and not args.cpu:
        model.esm = model.esm.half()

    model = model.eval()

    if torch.cuda.is_available() and not args.cpu:
        model = model.cuda()
        print(f"Using GPU: {torch.cuda.get_device_name(0)}")
    else:
        print("Using CPU (this will be slow)")

    print(f"Running inference on sequence ({len(args.sequence)} residues)...")
    tokenized = tokenizer([args.sequence], return_tensors="pt", add_special_tokens=False)["input_ids"]

    if not args.cpu and torch.cuda.is_available():
        tokenized = tokenized.cuda()

    with torch.no_grad():
        output = model(tokenized)

    pdb_string = convert_outputs_to_pdb(output)

    with open(args.output, "w") as f:
        f.write(pdb_string)

    mean_plddt = output["plddt"].mean().item()
    print(f"Structure saved to {args.output}")
    print(f"Mean pLDDT: {mean_plddt:.2f}")

if __name__ == "__main__":
    main()
INFEREOF
    chmod +x /workspace/inference.py

    # Create example FASTA
    mkdir -p /data/input /data/output
    cat > /data/input/example.fasta << 'FASTAEOF'
>example_protein
MKTVRQERLKSIVRILERSKEPVSGAQLAEELSVSRQVIVQDIAYLRSLGYNIVATPRGYVLAGG
>short_peptide
MKFLKFSLLTAVLLSVVFAFSSCGDDDDTYPYDVPDYAG
FASTAEOF

    # Clean up
    apt-get clean
    rm -rf /var/lib/apt/lists/*
    pip cache purge

%runscript
    exec python /workspace/test_installation.py "$@"

%test
    python -c "import torch; import transformers; import numpy; print('All imports successful')"

%help
    ESMFold HuggingFace Container
    =============================

    This container provides ESMFold protein structure prediction using
    HuggingFace Transformers (no OpenFold compilation required).

    Quick Start:
    -----------
    # Test installation (with GPU)
    singularity run --nv esmfold_hf.sif

    # Run inference on a sequence
    singularity exec --nv esmfold_hf.sif python /workspace/inference.py "MKTVRQERLK" --output test.pdb

    # With data binding
    singularity exec --nv --bind /path/to/data:/data esmfold_hf.sif \
        python /workspace/inference.py "YOURSEQUENCE" --output /data/output.pdb

    Environment:
    -----------
    - PyTorch 2.5.1 with CUDA 12.4
    - NumPy 1.26.4
    - HuggingFace Transformers

    Note: Model weights (~10GB) will be downloaded on first use.
    To cache them, bind a persistent directory:
    --bind /path/to/cache:/tmp/.cache/huggingface
