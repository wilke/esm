# ESMFold HuggingFace Dockerfile
#
# Modern, lightweight containerized environment for running ESMFold
# using HuggingFace Transformers (no OpenFold compilation required)
#
# Build command:
#   docker build -f Dockerfile.hf -t esmfold-hf:latest .
#
# Run command (with GPU):
#   docker run --gpus all -v $(pwd)/data:/data -it esmfold-hf:latest
#
# For CPU-only mode:
#   docker build -f Dockerfile.hf --build-arg INSTALL_GPU=false -t esmfold-hf:cpu .

# Use CUDA runtime (not devel) since we don't need compilation
FROM nvidia/cuda:12.1.0-runtime-ubuntu22.04

# Build arguments
ARG PYTHON_VERSION=3.10
ARG INSTALL_GPU=true
ARG TORCH_VERSION=2.2.0
ARG TORCH_CUDA=cu121

# Metadata
LABEL maintainer="ESM HuggingFace Docker Image"
LABEL description="ESMFold with HuggingFace Transformers - Lightweight Protein Structure Prediction"
LABEL version="2.0.1-hf"
LABEL org.opencontainers.image.source="https://github.com/facebookresearch/esm"

# Prevent interactive prompts during package installation
ENV DEBIAN_FRONTEND=noninteractive
ENV TZ=UTC
ENV PYTHONUNBUFFERED=1

# Set working directory
WORKDIR /workspace

# Install system dependencies (minimal set)
RUN apt-get update && apt-get install -y --no-install-recommends \
    python${PYTHON_VERSION} \
    python${PYTHON_VERSION}-dev \
    python3-pip \
    git \
    wget \
    curl \
    ca-certificates \
    && ln -sf /usr/bin/python${PYTHON_VERSION} /usr/bin/python \
    && ln -sf /usr/bin/python${PYTHON_VERSION} /usr/bin/python3 \
    && rm -rf /var/lib/apt/lists/*

# Upgrade pip
RUN python -m pip install --no-cache-dir --upgrade pip setuptools wheel

# Install PyTorch with appropriate CUDA support
RUN if [ "$INSTALL_GPU" = "true" ]; then \
        pip install --no-cache-dir \
        torch==${TORCH_VERSION} \
        --index-url https://download.pytorch.org/whl/${TORCH_CUDA}; \
    else \
        pip install --no-cache-dir torch==${TORCH_VERSION}; \
    fi

# Copy only requirements first for better layer caching
COPY requirements_hf.txt /workspace/

# Install HuggingFace dependencies
RUN pip install --no-cache-dir -r requirements_hf.txt

# Copy the ESM source code
COPY esm/ /workspace/esm/
COPY scripts/ /workspace/scripts/
COPY setup_hf.py /workspace/setup.py
COPY README.md /workspace/
COPY README_HF.md /workspace/
COPY LICENSE /workspace/

# Install ESM with HuggingFace support
RUN pip install --no-cache-dir -e ".[esmfold_hf]"

# Pre-download the model to bake it into the image (optional but recommended)
# Comment this out if you want to keep the image smaller
# RUN python -c "from transformers import EsmForProteinFolding; EsmForProteinFolding.from_pretrained('facebook/esmfold_v1')"

# Create directories for data
RUN mkdir -p /data/input /data/output /root/.cache/huggingface

# Add test script
RUN echo '#!/usr/bin/env python\n\
import sys\n\
import torch\n\
from transformers import EsmForProteinFolding, AutoTokenizer\n\
\n\
print("=" * 50)\n\
print("ESMFold HuggingFace Installation Test")\n\
print("=" * 50)\n\
print(f"Python version: {sys.version.split()[0]}")\n\
print(f"PyTorch version: {torch.__version__}")\n\
print(f"CUDA available: {torch.cuda.is_available()}")\n\
if torch.cuda.is_available():\n\
    print(f"CUDA version: {torch.version.cuda}")\n\
    print(f"GPU: {torch.cuda.get_device_name(0)}")\n\
    print(f"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB")\n\
\n\
print("\\nTesting HuggingFace transformers...")\n\
try:\n\
    from transformers import __version__ as tf_version\n\
    print(f"Transformers version: {tf_version}")\n\
    print("\\nLoading ESMFold tokenizer...")\n\
    tokenizer = AutoTokenizer.from_pretrained("facebook/esmfold_v1")\n\
    print("✓ Tokenizer loaded successfully!")\n\
    print("\\nNote: Model will be downloaded on first use (~10GB)")\n\
    print("Or pre-download with:")\n\
    print("  python -c \"from transformers import EsmForProteinFolding; EsmForProteinFolding.from_pretrained('"'"'facebook/esmfold_v1'"'"')\"") \n\
    print("\\n" + "=" * 50)\n\
    print("Installation test PASSED!")\n\
    print("=" * 50)\n\
except Exception as e:\n\
    print(f"\\n✗ Error: {e}")\n\
    sys.exit(1)\n\
' > /workspace/test_installation.py && chmod +x /workspace/test_installation.py

# Add simple inference script
RUN echo '#!/usr/bin/env python\n\
"""Simple ESMFold HF inference script\n\
Usage: python inference.py <sequence> [--output output.pdb]\n\
"""\n\
import argparse\n\
import torch\n\
from transformers import AutoTokenizer, EsmForProteinFolding\n\
from transformers.models.esm.openfold_utils.protein import to_pdb, Protein as OFProtein\n\
from transformers.models.esm.openfold_utils.feats import atom14_to_atom37\n\
\n\
def convert_outputs_to_pdb(outputs):\n\
    final_atom_positions = atom14_to_atom37(outputs["positions"][-1], outputs)\n\
    outputs = {k: v.to("cpu").numpy() for k, v in outputs.items()}\n\
    final_atom_positions = final_atom_positions.cpu().numpy()\n\
    final_atom_mask = outputs["atom37_atom_exists"]\n\
    \n\
    aa = outputs["aatype"][0]\n\
    pred_pos = final_atom_positions[0]\n\
    mask = final_atom_mask[0]\n\
    resid = outputs["residue_index"][0] + 1\n\
    pred = OFProtein(\n\
        aatype=aa,\n\
        atom_positions=pred_pos,\n\
        atom_mask=mask,\n\
        residue_index=resid,\n\
        b_factors=outputs["plddt"][0],\n\
        chain_index=outputs["chain_index"][0] if "chain_index" in outputs else None,\n\
    )\n\
    return to_pdb(pred)\n\
\n\
def main():\n\
    parser = argparse.ArgumentParser(description="Run ESMFold inference")\n\
    parser.add_argument("sequence", type=str, help="Protein sequence")\n\
    parser.add_argument("--output", type=str, default="output.pdb", help="Output PDB file")\n\
    parser.add_argument("--cpu", action="store_true", help="Use CPU instead of GPU")\n\
    parser.add_argument("--fp16", action="store_true", help="Use half precision")\n\
    args = parser.parse_args()\n\
    \n\
    print(f"Loading ESMFold model from HuggingFace...")\n\
    tokenizer = AutoTokenizer.from_pretrained("facebook/esmfold_v1")\n\
    model = EsmForProteinFolding.from_pretrained("facebook/esmfold_v1")\n\
    \n\
    if args.fp16 and not args.cpu:\n\
        model.esm = model.esm.half()\n\
    \n\
    model = model.eval()\n\
    \n\
    if torch.cuda.is_available() and not args.cpu:\n\
        model = model.cuda()\n\
        print(f"Using GPU: {torch.cuda.get_device_name(0)}")\n\
    else:\n\
        print("Using CPU (this will be slow)")\n\
    \n\
    print(f"Running inference on sequence ({len(args.sequence)} residues)...")\n\
    tokenized = tokenizer([args.sequence], return_tensors="pt", add_special_tokens=False)["input_ids"]\n\
    \n\
    if not args.cpu:\n\
        tokenized = tokenized.cuda()\n\
    \n\
    with torch.no_grad():\n\
        output = model(tokenized)\n\
    \n\
    pdb_string = convert_outputs_to_pdb(output)\n\
    \n\
    with open(args.output, "w") as f:\n\
        f.write(pdb_string)\n\
    \n\
    mean_plddt = output["plddt"].mean().item()\n\
    print(f"Structure saved to {args.output}")\n\
    print(f"Mean pLDDT: {mean_plddt:.2f}")\n\
\n\
if __name__ == "__main__":\n\
    main()\n\
' > /workspace/inference.py && chmod +x /workspace/inference.py

# Add CLI wrapper script
RUN echo '#!/bin/bash\n\
# Wrapper script for esm-fold-hf CLI\n\
# Usage: ./run_esmfold_hf.sh <input.fasta> <output_dir> [additional args]\n\
\n\
INPUT_FASTA=${1:-/data/input/example.fasta}\n\
OUTPUT_DIR=${2:-/data/output}\n\
shift 2\n\
\n\
echo "================================================"\n\
echo "ESMFold HuggingFace - Structure Prediction"\n\
echo "================================================"\n\
echo "Input: $INPUT_FASTA"\n\
echo "Output: $OUTPUT_DIR"\n\
echo ""\n\
\n\
# Check if GPU is available\n\
if python -c "import torch; exit(0 if torch.cuda.is_available() else 1)" 2>/dev/null; then\n\
    echo "GPU detected: $(python -c \"import torch; print(torch.cuda.get_device_name(0))\")")\n\
    GPU_ARGS="--fp16 --use-tf32"\n\
else\n\
    echo "No GPU detected, using CPU mode"\n\
    GPU_ARGS="--cpu-only"\n\
fi\n\
\n\
echo ""\n\
echo "Running esm-fold-hf..."\n\
echo "================================================"\n\
\n\
esm-fold-hf \\\n\
    -i "$INPUT_FASTA" \\\n\
    -o "$OUTPUT_DIR" \\\n\
    $GPU_ARGS \\\n\
    "$@"\n\
\n\
echo ""\n\
echo "================================================"\n\
echo "Done! PDB files saved to $OUTPUT_DIR"\n\
echo "================================================"\n\
' > /workspace/run_esmfold_hf.sh && chmod +x /workspace/run_esmfold_hf.sh

# Create example FASTA file
RUN echo '>example_protein\n\
MKTVRQERLKSIVRILERSKEPVSGAQLAEELSVSRQVIVQDIAYLRSLGYNIVATPRGYVLAGG\n\
>short_peptide\n\
MKFLKFSLLTAVLLSVVFAFSSCGDDDDTYPYDVPDYAG\n\
' > /data/input/example.fasta

# Add helpful README
RUN echo 'ESMFold HuggingFace Docker Container\n\
=====================================\n\
\n\
Quick Start:\n\
------------\n\
\n\
1. Test installation:\n\
   python /workspace/test_installation.py\n\
\n\
2. Run on example sequences:\n\
   esm-fold-hf -i /data/input/example.fasta -o /data/output\n\
\n\
3. Use the wrapper script:\n\
   /workspace/run_esmfold_hf.sh /data/input/example.fasta /data/output\n\
\n\
4. Run inference on a single sequence:\n\
   python /workspace/inference.py "MKTVRQERLK" --output protein.pdb\n\
\n\
Mount your data:\n\
----------------\n\
docker run --gpus all \\\n\
  -v /path/to/input:/data/input \\\n\
  -v /path/to/output:/data/output \\\n\
  esmfold-hf:latest \\\n\
  esm-fold-hf -i /data/input/sequences.fasta -o /data/output\n\
\n\
Memory optimization:\n\
--------------------\n\
For large proteins or limited GPU memory:\n\
  esm-fold-hf -i input.fasta -o output/ --fp16 --chunk-size 64\n\
\n\
For CPU-only:\n\
  esm-fold-hf -i input.fasta -o output/ --cpu-only\n\
' > /workspace/README.txt

# Set environment variables
ENV CUDA_VISIBLE_DEVICES=0
ENV HF_HOME=/root/.cache/huggingface
ENV TRANSFORMERS_CACHE=/root/.cache/huggingface/transformers

# Volume mount points
VOLUME ["/data/input", "/data/output", "/root/.cache/huggingface"]

# Default command - show installation test
CMD ["python", "/workspace/test_installation.py"]

# Health check
HEALTHCHECK --interval=30s --timeout=10s --start-period=5s --retries=3 \
    CMD python -c "import torch; from transformers import AutoTokenizer; AutoTokenizer.from_pretrained('facebook/esmfold_v1')" || exit 1
